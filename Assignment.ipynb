{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88415d32",
        "outputId": "ec3a1ab7-77f3-45f3-9006-88b9116c54e4"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,795 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,266 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,273 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Fetched 24.3 MB in 3s (9,080 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.10 [186 kB]\n",
            "Fetched 186 kB in 1s (277 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.10_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.10) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d0be83"
      },
      "source": [
        "# Task\n",
        "Create a Python script that loads a PDF, splits it into chunks, creates a Pinecone vector store from the chunks using a custom embedding model, sets up a multi-query retriever and a conversational retrieval chain with memory, and includes a function to clean the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47544605"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install all the required Python libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4abb69"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install the required libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0bf3e85"
      },
      "source": [
        "!pip install --q langchain_openai\n",
        "!pip install --q langchain\n",
        "!pip install --q unstructured langchain\n",
        "!pip install --q \"unstructured[all-docs]\"\n",
        "!pip install --q langchain_community\n",
        "!pip install --q chromadb\n",
        "!pip install --q langchain_text_splitters\n",
        "!pip install --q poppler-utils\n",
        "!pip install --q pinecone\n",
        "!pip install --q langchain_pinecone"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7898ca"
      },
      "source": [
        "## Set up environment variables\n",
        "\n",
        "### Subtask:\n",
        "Set up the API keys for OpenAI and Pinecone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a43fd7be"
      },
      "source": [
        "**Reasoning**:\n",
        "Set the environment variables for OpenAI and Pinecone API keys and the OpenAI API base URL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9853da8d"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\"\n",
        "os.environ['OPENAI_API_BASE'] = \"https://openrouter.ai/api/v1\"\n",
        "os.environ['PINECONE_API_KEY'] = \"pcsk_74vokm_KUuxFinVGUYHVxjtpyxwMixzQVV9whb7TCyUpUs1JjPkxupr95GShq4Fx8e6USz\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51e948ce"
      },
      "source": [
        "## Define embedding model\n",
        "\n",
        "### Subtask:\n",
        "Define the custom embedding model using SentenceTransformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb1ef6de"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the custom embedding model class and instantiate it as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360c7762"
      },
      "source": [
        "from langchain_core.embeddings import Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List\n",
        "\n",
        "class SentenceTransformerEmbeddingsWrapper(Embeddings):\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.model.encode(text).tolist()\n",
        "\n",
        "model_name = \"latterworks/ollama-embeddings\"\n",
        "embeddings_model = SentenceTransformerEmbeddingsWrapper(model_name)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "025c207d"
      },
      "source": [
        "## Load and split pdf\n",
        "\n",
        "### Subtask:\n",
        "Load the PDF document and split it into smaller chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8c48d92"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the PDF and split it into chunks according to the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "470d9a9c",
        "outputId": "59473142-f9a5-4026-881b-db7481582583"
      },
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=100)\n",
        "pdf_paths = [\"/content/Persuader PDF.pdf\"]\n",
        "all_chunks = []\n",
        "\n",
        "for pdf_path in pdf_paths:\n",
        "    print(f\"Processing {pdf_path}...\")\n",
        "    loader = UnstructuredPDFLoader(pdf_path)\n",
        "    data = loader.load()\n",
        "    chunks = splitter.split_documents(data)\n",
        "    for chunk in chunks:\n",
        "        chunk.metadata[\"source_file\"] = pdf_path\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "print(f\"Processed {len(pdf_paths)} PDF(s) and created {len(all_chunks)} chunks.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/Persuader PDF.pdf...\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Processed 1 PDF(s) and created 17 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a25fb7f7"
      },
      "source": [
        "## Initialize pinecone\n",
        "\n",
        "### Subtask:\n",
        "Initialize the Pinecone client and define the index name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8fcb269"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the Pinecone client and define the index name as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48fd35ed",
        "outputId": "11c6c9be-9057-41ff-adaa-ec350e61e873"
      },
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# 1. Initialize Pinecone client\n",
        "api_key = os.environ.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=api_key)\n",
        "\n",
        "# 2. Define the index name\n",
        "index_name = \"apirag\"\n",
        "\n",
        "print(f\"Pinecone client initialized. Index name set to '{index_name}'.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone client initialized. Index name set to 'apirag'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecc9feff"
      },
      "source": [
        "## Create pinecone vector store\n",
        "\n",
        "### Subtask:\n",
        "Create a Pinecone vector store from the document chunks and embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4712f548"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a Pinecone vector store from the document chunks and embeddings using the PineconeVectorStore.from_documents method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f714ae6d",
        "outputId": "8aba4d32-f1cb-455f-eb9f-00dff8ed1513"
      },
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore.from_documents(\n",
        "    all_chunks,\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings_model\n",
        ")\n",
        "\n",
        "print(\"Pinecone vector store created successfully.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone vector store created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0524b0e0"
      },
      "source": [
        "## Set up multiqueryretriever\n",
        "\n",
        "### Subtask:\n",
        "Set up a retriever that generates multiple queries from the user's question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47a8e5a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the prompt template for generating multiple queries and set up the MultiQueryRetriever using the vector store and LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25498b56"
      },
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "query_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate\n",
        "     five different versions of the given user question to retrieve relevant documents\n",
        "    from a vector database. By generating multiple perspectives on the user question,\n",
        "    your goal is to help the user overcome some of the limitations of the distance\n",
        "    based similarity search. Provide these alternative questions separated by new lines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    vector_store.as_retriever(),\n",
        "    llm,\n",
        "    prompt=query_prompt,\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5183d640"
      },
      "source": [
        "## Create conversationalretrievalchain\n",
        "\n",
        "### Subtask:\n",
        "Create a RAG chain that uses the retriever and an LLM to answer questions with memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0f06f8"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes for the conversational retrieval chain and define the RAG prompt template.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07a0cffd"
      },
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "#Rag prompt\n",
        "template = \"\"\"Answer the question based only on the following context and also give source snippets from the pdf below the answer:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c20707"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the conversation memory and create the conversational retrieval chain with memory, then define the separate chain without memory for direct querying.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aeccec1"
      },
      "source": [
        "# Initialize ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Create a conversational retrieval chain\n",
        "chain_with_memory = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "# The original chain without memory is still defined here for reference\n",
        "chain = (\n",
        "    {\"context\" : retriever, \"question\" : RunnablePassthrough()}\n",
        "    | prompt | llm | StrOutputParser()\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d79328a"
      },
      "source": [
        "## Define output cleaning function\n",
        "\n",
        "### Subtask:\n",
        "Define a function to clean the output string.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b499fc2"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the clean_output function as instructed to format the output string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c5f198b"
      },
      "source": [
        "def clean_output(output_string):\n",
        "    # Replace '\\n\\n' with actual newlines and format list items\n",
        "    formatted_output = output_string.replace('\\n\\n', '\\n')\n",
        "    lines = formatted_output.split('\\n')\n",
        "    processed_lines = []\n",
        "    for line in lines:\n",
        "        # Remove '*' and leading/trailing spaces\n",
        "        processed_line = line.lstrip('* ').strip()\n",
        "        processed_lines.append(processed_line)\n",
        "\n",
        "    final_output = '\\n'.join(processed_lines)\n",
        "    return final_output"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1640a41b"
      },
      "source": [
        "# Task\n",
        "Integrate a Pinecone reranker into the existing LangChain conversational retrieval chain and the runnable chain, ensuring the reranking step occurs after document retrieval and before answer generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5b749c9"
      },
      "source": [
        "## Initialize pinecone reranker\n",
        "\n",
        "### Subtask:\n",
        "Initialize the Pinecone reranker model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1cbff1"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the Pinecone reranker model using the Pinecone API key and specify the number of top results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bab18a45",
        "outputId": "3a8cad36-bd0c-4018-f728-357d14cac7fc"
      },
      "source": [
        "from langchain_pinecone import PineconeRerank\n",
        "\n",
        "# Assuming PINECONE_API_KEY is already set as an environment variable\n",
        "reranker = PineconeRerank(api_key=os.environ.get('PINECONE_API_KEY'), top_n=5)\n",
        "\n",
        "print(\"Pinecone reranker initialized.\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone reranker initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21c5ac3c"
      },
      "source": [
        "## Modify the rag chain\n",
        "\n",
        "### Subtask:\n",
        "Update the existing conversational retrieval chain to include the reranking step after document retrieval and before answer generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f03d356d"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the existing conversational retrieval chain to include the reranking step after document retrieval and before answer generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMis4I1KQT5y",
        "outputId": "0d8c1b74-d0a6-4589-ebe1-c16345055f56"
      },
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Define a custom runnable that retrieves documents and then reranks them\n",
        "def retrieve_and_rerank(input_dict):\n",
        "    question = input_dict[\"question\"]\n",
        "    chat_history = input_dict[\"chat_history\"]\n",
        "\n",
        "    # Get relevant documents using the base retriever\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Rerank the retrieved documents\n",
        "    reranked_docs = reranker.rerank(retrieved_docs, question)\n",
        "\n",
        "    return {\"question\": question, \"context\": reranked_docs, \"chat_history\": chat_history}\n",
        "\n",
        "# Create a new conversational retrieval chain with the custom runnable\n",
        "chain_with_memory_reranked = (\n",
        "    RunnableLambda(retrieve_and_rerank)\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Conversational retrieval chain with reranking created successfully using a custom runnable.\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversational retrieval chain with reranking created successfully using a custom runnable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a1acac"
      },
      "source": [
        "## Update the runnable chain\n",
        "\n",
        "### Subtask:\n",
        "Update the runnable chain to include the reranking step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1d37b4"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the existing runnable chain to include the reranking step by inserting the reranker after the retriever and before the prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d936d38"
      },
      "source": [
        "**Reasoning**:\n",
        "The `PineconeRerank` object is not a standard Runnable and cannot be directly chained using the `|` operator. A custom runnable or a LangChain Expression Language (LCEL) chain is needed to integrate the reranking step. Since the goal is to modify the *existing* runnable chain structure, a `RunnableLambda` can be used to wrap the retrieval and reranking logic, similar to how it was done for the conversational chain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1okM1Bn_Qbdy",
        "outputId": "8afe3316-4a20-4934-80f1-e8e55258a10e"
      },
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Update the original chain without memory to include the reranker\n",
        "# Use a RunnableLambda to wrap the retriever and reranker\n",
        "chain = (\n",
        "    {\"context\": RunnableLambda(lambda x: reranker.rerank(retriever.invoke(x['question']), x['question'])),\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | prompt | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Runnable chain updated successfully to include the reranker using RunnableLambda.\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runnable chain updated successfully to include the reranker using RunnableLambda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7eda4c0",
        "outputId": "0b538c97-6743-41bb-e0e3-16bd02d5dafb"
      },
      "source": [
        "# Get user input\n",
        "user_question = input(\"Enter your question about the document: \")\n",
        "\n",
        "# Invoke the conversational retrieval chain with the user's question\n",
        "# Assuming the reranked chain is named 'chain_with_memory_reranked' or similar after the previous steps\n",
        "# If you want to use the original chain without memory but with reranking, use 'chain'\n",
        "# For this example, let's use the conversational chain with reranking\n",
        "result = chain_with_memory_reranked.invoke({\"question\": user_question, \"chat_history\": memory.load_memory_variables({})['chat_history']})\n",
        "\n",
        "\n",
        "# Clean the output\n",
        "cleaned_answer = clean_output(result)\n",
        "\n",
        "# Print the cleaned answer\n",
        "print(\"\\nAnswer:\")\n",
        "print(cleaned_answer)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question about the document: which character is kind of an antihero in the novel\n",
            "\n",
            "Answer:\n",
            "Answer**\n",
            "The character who functions as the novel’s anti‑hero is **Jack Reacher**.\n",
            "Reacher is a former military policeman who operates largely outside the law, yet he is driven by a personal code of honor and a strong sense of empathy toward those he protects. This blend of violent capability and moral complexity is what makes him an anti‑hero.\n",
            "Source snippets**\n",
            "- “The protagonist’s emotional reaction to the aftermath of violence signifies a deep sense of empathy and respect for human life, even amidst his violent world.”\n",
            "(Doc 0, Q&A section)\n",
            "- “The protagonist’s willingness to face personal danger in order to help his colleagues and seek justice for the fallen reflects a profound commitment to personal sacrifice.”\n",
            "(Doc 0, Q&A section)\n",
            "These passages highlight Reacher’s moral ambiguity and his willingness to act outside conventional law‑making, hallmarks of an anti‑hero.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fUeskv6IRTn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}